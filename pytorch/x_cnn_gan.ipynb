{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision.utils import save_image, make_grid\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import rcParams\n",
    "rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "num_epochs = 100\n",
    "\n",
    "z_dimension = 100\n",
    "num_feature = 56 * 56"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((.5, .5, .5), (.5, .5, .5))\n",
    "])\n",
    "\n",
    "dataset = datasets.MNIST('./datas', transform=img_transform)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, padding=1),\n",
    "            nn.LeakyReLU(.2, True),\n",
    "            nn.AvgPool2d(2, 2), \n",
    "        ) # b 32 14 14\n",
    "        \n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.LeakyReLU(.2, True),\n",
    "            nn.AvgPool2d(2, 2),\n",
    "        ) # b 64 7 7\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(64 * 7 * 7, 1024),\n",
    "            nn.LeakyReLU(.2, True),\n",
    "            nn.Linear(1024, 1),\n",
    "            nn.Sigmoid(),\n",
    "        ) # b 1\n",
    "    \n",
    "    def forward(self, x): # b 1 28 28\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2(out)\n",
    "        \n",
    "        out = out.view(x.size(0), -1)\n",
    "        return self.fc(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, inp_dim, num_feature):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(inp_dim, num_feature)\n",
    "        ) # b h*w\n",
    "        self.br = nn.Sequential(\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.LeakyReLU(.2, True),\n",
    "        ) # b 1 56 56\n",
    "        \n",
    "        self.downsample1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(.2, True),\n",
    "        ) # b 64 56 56\n",
    "        \n",
    "        self.downsample2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 32, 3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(.2, True),\n",
    "        ) # b 32 56 56\n",
    "        \n",
    "        self.downsample3 = nn.Sequential(\n",
    "            nn.Conv2d(32, 1, 3, padding=1, stride=2),\n",
    "            nn.Tanh(),\n",
    "        ) # b 1 28 28\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.fc(x)\n",
    "        \n",
    "        out = out.view(x.size(0), 1, 56, 56)\n",
    "        out = self.br(out)\n",
    "        out = self.downsample1(out)\n",
    "        out = self.downsample2(out)\n",
    "        out = self.downsample3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = Discriminator().to(device)\n",
    "g = Generator(z_dimension, num_feature).to(device)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "d_optimezer = optim.Adam(d.parameters(), lr=1e-4)\n",
    "g_optimezer = optim.Adam(g.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter('./log/cnn_gan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Step:  16000, d_loss: 0.447707, g_loss: 0.972548, real_scores: 0.875926, fake_scores: 0.220894\n",
      "Epoch [1/100], Step:  32000, d_loss: 0.588346, g_loss: 1.651088, real_scores: 0.758777, fake_scores: 0.218019\n",
      "Epoch [1/100], Step:  48000, d_loss: 0.422349, g_loss: 2.128111, real_scores: 0.835960, fake_scores: 0.181344\n",
      "Epoch [2/100], Step:  16000, d_loss: 0.659415, g_loss: 2.391714, real_scores: 0.952546, fake_scores: 0.383948\n",
      "Epoch [2/100], Step:  32000, d_loss: 0.293448, g_loss: 2.903997, real_scores: 0.923110, fake_scores: 0.150144\n",
      "Epoch [2/100], Step:  48000, d_loss: 0.321554, g_loss: 1.613950, real_scores: 0.932759, fake_scores: 0.177828\n",
      "Epoch [3/100], Step:  16000, d_loss: 0.462831, g_loss: 3.195925, real_scores: 0.914801, fake_scores: 0.256083\n",
      "Epoch [3/100], Step:  32000, d_loss: 0.469265, g_loss: 1.662153, real_scores: 0.800443, fake_scores: 0.080927\n",
      "Epoch [3/100], Step:  48000, d_loss: 0.300416, g_loss: 2.891566, real_scores: 0.870517, fake_scores: 0.106174\n",
      "Epoch [4/100], Step:  16000, d_loss: 0.366949, g_loss: 2.489497, real_scores: 0.890008, fake_scores: 0.173648\n",
      "Epoch [4/100], Step:  32000, d_loss: 0.678346, g_loss: 2.416962, real_scores: 0.875126, fake_scores: 0.313763\n",
      "Epoch [4/100], Step:  48000, d_loss: 0.538794, g_loss: 2.608331, real_scores: 0.906638, fake_scores: 0.247036\n",
      "Epoch [5/100], Step:  16000, d_loss: 0.357596, g_loss: 3.360862, real_scores: 0.879573, fake_scores: 0.097044\n",
      "Epoch [5/100], Step:  32000, d_loss: 0.289779, g_loss: 3.641419, real_scores: 0.866291, fake_scores: 0.073934\n",
      "Epoch [5/100], Step:  48000, d_loss: 0.563412, g_loss: 2.877171, real_scores: 0.753265, fake_scores: 0.086763\n",
      "Epoch [6/100], Step:  16000, d_loss: 0.386292, g_loss: 2.946601, real_scores: 0.874144, fake_scores: 0.150953\n",
      "Epoch [6/100], Step:  32000, d_loss: 0.493818, g_loss: 2.363015, real_scores: 0.930677, fake_scores: 0.285896\n",
      "Epoch [6/100], Step:  48000, d_loss: 0.454787, g_loss: 3.212734, real_scores: 0.879408, fake_scores: 0.143382\n",
      "Epoch [7/100], Step:  16000, d_loss: 0.400496, g_loss: 2.184206, real_scores: 0.920371, fake_scores: 0.196086\n",
      "Epoch [7/100], Step:  32000, d_loss: 0.429101, g_loss: 2.694391, real_scores: 0.917902, fake_scores: 0.251563\n",
      "Epoch [7/100], Step:  48000, d_loss: 0.366888, g_loss: 1.478604, real_scores: 0.903962, fake_scores: 0.145658\n",
      "Epoch [8/100], Step:  16000, d_loss: 0.429654, g_loss: 2.712056, real_scores: 0.812496, fake_scores: 0.083504\n",
      "Epoch [8/100], Step:  32000, d_loss: 0.377678, g_loss: 2.625290, real_scores: 0.894550, fake_scores: 0.115582\n",
      "Epoch [8/100], Step:  48000, d_loss: 0.258716, g_loss: 2.786381, real_scores: 0.906356, fake_scores: 0.110097\n",
      "Epoch [9/100], Step:  16000, d_loss: 0.251146, g_loss: 2.261522, real_scores: 0.950544, fake_scores: 0.148892\n",
      "Epoch [9/100], Step:  32000, d_loss: 0.577485, g_loss: 2.476443, real_scores: 0.858435, fake_scores: 0.154786\n",
      "Epoch [9/100], Step:  48000, d_loss: 0.325057, g_loss: 1.899447, real_scores: 0.940128, fake_scores: 0.199762\n",
      "Epoch [10/100], Step:  16000, d_loss: 0.430206, g_loss: 2.775756, real_scores: 0.872287, fake_scores: 0.161666\n",
      "Epoch [10/100], Step:  32000, d_loss: 0.487728, g_loss: 3.425340, real_scores: 0.806053, fake_scores: 0.075044\n",
      "Epoch [10/100], Step:  48000, d_loss: 0.505154, g_loss: 2.285108, real_scores: 0.936568, fake_scores: 0.221314\n",
      "Epoch [11/100], Step:  16000, d_loss: 0.364617, g_loss: 2.896314, real_scores: 0.865489, fake_scores: 0.133641\n",
      "Epoch [11/100], Step:  32000, d_loss: 0.276763, g_loss: 3.116665, real_scores: 0.901635, fake_scores: 0.122562\n",
      "Epoch [11/100], Step:  48000, d_loss: 0.290207, g_loss: 2.467255, real_scores: 0.915636, fake_scores: 0.160454\n",
      "Epoch [12/100], Step:  16000, d_loss: 0.223700, g_loss: 3.167911, real_scores: 0.946221, fake_scores: 0.125050\n",
      "Epoch [12/100], Step:  32000, d_loss: 0.407288, g_loss: 2.357593, real_scores: 0.948003, fake_scores: 0.201135\n",
      "Epoch [12/100], Step:  48000, d_loss: 0.477247, g_loss: 2.574221, real_scores: 0.874445, fake_scores: 0.171341\n",
      "Epoch [13/100], Step:  16000, d_loss: 0.540899, g_loss: 2.084150, real_scores: 0.868161, fake_scores: 0.195624\n",
      "Epoch [13/100], Step:  32000, d_loss: 0.405452, g_loss: 4.024101, real_scores: 0.824766, fake_scores: 0.086979\n",
      "Epoch [13/100], Step:  48000, d_loss: 0.451564, g_loss: 2.433780, real_scores: 0.876445, fake_scores: 0.130044\n",
      "Epoch [14/100], Step:  16000, d_loss: 0.553769, g_loss: 1.993788, real_scores: 0.861148, fake_scores: 0.194153\n",
      "Epoch [14/100], Step:  32000, d_loss: 0.315124, g_loss: 3.161957, real_scores: 0.844615, fake_scores: 0.074305\n",
      "Epoch [14/100], Step:  48000, d_loss: 0.415338, g_loss: 1.856196, real_scores: 0.876104, fake_scores: 0.158465\n",
      "Epoch [15/100], Step:  16000, d_loss: 0.321390, g_loss: 2.143509, real_scores: 0.896876, fake_scores: 0.108853\n",
      "Epoch [15/100], Step:  32000, d_loss: 0.397822, g_loss: 1.721168, real_scores: 0.853527, fake_scores: 0.147663\n",
      "Epoch [15/100], Step:  48000, d_loss: 0.406612, g_loss: 3.021884, real_scores: 0.839578, fake_scores: 0.125451\n",
      "Epoch [16/100], Step:  16000, d_loss: 0.482216, g_loss: 2.963864, real_scores: 0.871168, fake_scores: 0.152838\n",
      "Epoch [16/100], Step:  32000, d_loss: 0.535635, g_loss: 2.084317, real_scores: 0.806967, fake_scores: 0.085052\n",
      "Epoch [16/100], Step:  48000, d_loss: 0.321843, g_loss: 2.833452, real_scores: 0.897273, fake_scores: 0.109332\n",
      "Epoch [17/100], Step:  16000, d_loss: 0.334039, g_loss: 3.018248, real_scores: 0.883104, fake_scores: 0.090484\n",
      "Epoch [17/100], Step:  32000, d_loss: 0.307600, g_loss: 3.510356, real_scores: 0.900385, fake_scores: 0.130416\n",
      "Epoch [17/100], Step:  48000, d_loss: 0.479786, g_loss: 4.082256, real_scores: 0.834552, fake_scores: 0.112738\n",
      "Epoch [18/100], Step:  16000, d_loss: 0.451882, g_loss: 2.684213, real_scores: 0.863139, fake_scores: 0.159956\n",
      "Epoch [18/100], Step:  32000, d_loss: 0.252421, g_loss: 3.022851, real_scores: 0.898399, fake_scores: 0.069394\n",
      "Epoch [18/100], Step:  48000, d_loss: 0.564466, g_loss: 3.621427, real_scores: 0.771974, fake_scores: 0.042683\n",
      "Epoch [19/100], Step:  16000, d_loss: 0.257711, g_loss: 3.292299, real_scores: 0.889095, fake_scores: 0.089578\n",
      "Epoch [19/100], Step:  32000, d_loss: 0.401192, g_loss: 2.800123, real_scores: 0.826007, fake_scores: 0.067501\n",
      "Epoch [19/100], Step:  48000, d_loss: 0.478671, g_loss: 2.465651, real_scores: 0.877467, fake_scores: 0.152299\n",
      "Epoch [20/100], Step:  16000, d_loss: 0.339375, g_loss: 2.563734, real_scores: 0.831495, fake_scores: 0.055286\n",
      "Epoch [20/100], Step:  32000, d_loss: 0.178821, g_loss: 2.771056, real_scores: 0.945625, fake_scores: 0.099597\n",
      "Epoch [20/100], Step:  48000, d_loss: 0.383108, g_loss: 3.518173, real_scores: 0.892549, fake_scores: 0.128582\n",
      "Epoch [21/100], Step:  16000, d_loss: 0.427188, g_loss: 1.762213, real_scores: 0.858911, fake_scores: 0.170288\n",
      "Epoch [21/100], Step:  32000, d_loss: 0.429906, g_loss: 1.948984, real_scores: 0.839276, fake_scores: 0.123655\n",
      "Epoch [21/100], Step:  48000, d_loss: 0.306977, g_loss: 2.623663, real_scores: 0.896421, fake_scores: 0.133315\n",
      "Epoch [22/100], Step:  16000, d_loss: 0.519934, g_loss: 2.384896, real_scores: 0.868473, fake_scores: 0.201254\n",
      "Epoch [22/100], Step:  32000, d_loss: 0.354035, g_loss: 2.495901, real_scores: 0.888450, fake_scores: 0.110760\n",
      "Epoch [22/100], Step:  48000, d_loss: 0.563421, g_loss: 1.891365, real_scores: 0.968388, fake_scores: 0.346902\n",
      "Epoch [23/100], Step:  16000, d_loss: 0.486348, g_loss: 2.816057, real_scores: 0.827432, fake_scores: 0.113797\n",
      "Epoch [23/100], Step:  32000, d_loss: 0.562452, g_loss: 2.138603, real_scores: 0.797816, fake_scores: 0.138364\n",
      "Epoch [23/100], Step:  48000, d_loss: 0.936502, g_loss: 3.293349, real_scores: 0.718343, fake_scores: 0.051653\n",
      "Epoch [24/100], Step:  16000, d_loss: 0.413014, g_loss: 2.661556, real_scores: 0.882105, fake_scores: 0.164532\n",
      "Epoch [24/100], Step:  32000, d_loss: 0.475833, g_loss: 3.450729, real_scores: 0.811478, fake_scores: 0.085550\n",
      "Epoch [24/100], Step:  48000, d_loss: 0.543136, g_loss: 2.869224, real_scores: 0.795316, fake_scores: 0.063464\n",
      "Epoch [25/100], Step:  16000, d_loss: 0.303331, g_loss: 2.237405, real_scores: 0.948873, fake_scores: 0.160910\n",
      "Epoch [25/100], Step:  32000, d_loss: 0.361245, g_loss: 1.817878, real_scores: 0.920954, fake_scores: 0.185739\n",
      "Epoch [25/100], Step:  48000, d_loss: 0.357006, g_loss: 3.112209, real_scores: 0.851579, fake_scores: 0.047801\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [26/100], Step:  16000, d_loss: 0.512827, g_loss: 3.295505, real_scores: 0.839873, fake_scores: 0.078204\n",
      "Epoch [26/100], Step:  32000, d_loss: 0.477461, g_loss: 4.188879, real_scores: 0.846053, fake_scores: 0.038849\n",
      "Epoch [26/100], Step:  48000, d_loss: 0.488970, g_loss: 2.920677, real_scores: 0.797888, fake_scores: 0.071363\n",
      "Epoch [27/100], Step:  16000, d_loss: 0.784063, g_loss: 2.546706, real_scores: 0.823910, fake_scores: 0.262032\n",
      "Epoch [27/100], Step:  32000, d_loss: 0.307745, g_loss: 2.138925, real_scores: 0.947818, fake_scores: 0.188435\n",
      "Epoch [27/100], Step:  48000, d_loss: 0.475131, g_loss: 3.361176, real_scores: 0.800968, fake_scores: 0.055124\n",
      "Epoch [28/100], Step:  16000, d_loss: 0.393286, g_loss: 3.425052, real_scores: 0.835607, fake_scores: 0.057526\n",
      "Epoch [28/100], Step:  32000, d_loss: 0.405798, g_loss: 2.640990, real_scores: 0.848358, fake_scores: 0.127495\n",
      "Epoch [28/100], Step:  48000, d_loss: 0.200334, g_loss: 3.211935, real_scores: 0.925625, fake_scores: 0.067133\n",
      "Epoch [29/100], Step:  16000, d_loss: 0.414563, g_loss: 2.259236, real_scores: 0.928870, fake_scores: 0.238440\n",
      "Epoch [29/100], Step:  32000, d_loss: 0.287615, g_loss: 2.312424, real_scores: 0.950114, fake_scores: 0.187262\n",
      "Epoch [29/100], Step:  48000, d_loss: 0.153555, g_loss: 2.656891, real_scores: 0.967896, fake_scores: 0.105417\n",
      "Epoch [30/100], Step:  16000, d_loss: 0.602418, g_loss: 2.661074, real_scores: 0.808125, fake_scores: 0.081649\n",
      "Epoch [30/100], Step:  32000, d_loss: 0.466971, g_loss: 2.149268, real_scores: 0.891456, fake_scores: 0.201886\n",
      "Epoch [30/100], Step:  48000, d_loss: 0.278213, g_loss: 2.587231, real_scores: 0.930701, fake_scores: 0.134709\n",
      "Epoch [31/100], Step:  16000, d_loss: 0.775295, g_loss: 2.385488, real_scores: 0.716723, fake_scores: 0.066003\n",
      "Epoch [31/100], Step:  32000, d_loss: 0.444881, g_loss: 2.669891, real_scores: 0.857200, fake_scores: 0.110570\n",
      "Epoch [31/100], Step:  48000, d_loss: 0.328295, g_loss: 3.111734, real_scores: 0.881685, fake_scores: 0.067428\n",
      "Epoch [32/100], Step:  16000, d_loss: 0.580001, g_loss: 1.628727, real_scores: 0.798169, fake_scores: 0.179443\n",
      "Epoch [32/100], Step:  32000, d_loss: 0.359310, g_loss: 2.532042, real_scores: 0.858776, fake_scores: 0.066156\n",
      "Epoch [32/100], Step:  48000, d_loss: 0.401131, g_loss: 1.631492, real_scores: 0.900053, fake_scores: 0.192617\n",
      "Epoch [33/100], Step:  16000, d_loss: 0.350432, g_loss: 1.879141, real_scores: 0.901053, fake_scores: 0.127605\n",
      "Epoch [33/100], Step:  32000, d_loss: 0.337577, g_loss: 1.852794, real_scores: 0.888451, fake_scores: 0.133063\n",
      "Epoch [33/100], Step:  48000, d_loss: 0.615627, g_loss: 2.548430, real_scores: 0.821228, fake_scores: 0.167613\n",
      "Epoch [34/100], Step:  16000, d_loss: 0.432876, g_loss: 2.451437, real_scores: 0.875725, fake_scores: 0.158857\n",
      "Epoch [34/100], Step:  32000, d_loss: 0.324922, g_loss: 2.498654, real_scores: 0.872740, fake_scores: 0.075038\n",
      "Epoch [34/100], Step:  48000, d_loss: 0.317899, g_loss: 2.545488, real_scores: 0.932839, fake_scores: 0.154043\n",
      "Epoch [35/100], Step:  16000, d_loss: 0.183931, g_loss: 2.795392, real_scores: 0.967308, fake_scores: 0.114211\n",
      "Epoch [35/100], Step:  32000, d_loss: 0.570919, g_loss: 1.980701, real_scores: 0.844668, fake_scores: 0.219545\n",
      "Epoch [35/100], Step:  48000, d_loss: 0.350843, g_loss: 3.618937, real_scores: 0.915996, fake_scores: 0.081015\n",
      "Epoch [36/100], Step:  16000, d_loss: 0.609749, g_loss: 2.266868, real_scores: 0.800688, fake_scores: 0.194760\n",
      "Epoch [36/100], Step:  32000, d_loss: 0.394342, g_loss: 2.004543, real_scores: 0.895452, fake_scores: 0.167239\n",
      "Epoch [36/100], Step:  48000, d_loss: 0.243341, g_loss: 2.996223, real_scores: 0.932627, fake_scores: 0.086237\n",
      "Epoch [37/100], Step:  16000, d_loss: 0.261955, g_loss: 2.512303, real_scores: 0.908271, fake_scores: 0.077081\n",
      "Epoch [37/100], Step:  32000, d_loss: 0.383918, g_loss: 2.477816, real_scores: 0.943791, fake_scores: 0.217267\n",
      "Epoch [37/100], Step:  48000, d_loss: 0.435078, g_loss: 2.569256, real_scores: 0.844117, fake_scores: 0.118600\n",
      "Epoch [38/100], Step:  16000, d_loss: 0.372604, g_loss: 2.261530, real_scores: 0.880639, fake_scores: 0.134507\n",
      "Epoch [38/100], Step:  32000, d_loss: 0.472906, g_loss: 2.196940, real_scores: 0.863105, fake_scores: 0.135179\n",
      "Epoch [38/100], Step:  48000, d_loss: 0.484189, g_loss: 2.884353, real_scores: 0.829455, fake_scores: 0.055615\n",
      "Epoch [39/100], Step:  16000, d_loss: 0.407694, g_loss: 2.097554, real_scores: 0.921377, fake_scores: 0.219627\n",
      "Epoch [39/100], Step:  32000, d_loss: 0.399064, g_loss: 2.534472, real_scores: 0.870445, fake_scores: 0.136864\n",
      "Epoch [39/100], Step:  48000, d_loss: 0.409976, g_loss: 2.620697, real_scores: 0.841911, fake_scores: 0.095312\n",
      "Epoch [40/100], Step:  16000, d_loss: 0.328772, g_loss: 2.757574, real_scores: 0.896905, fake_scores: 0.129015\n",
      "Epoch [40/100], Step:  32000, d_loss: 0.396928, g_loss: 2.024002, real_scores: 0.882449, fake_scores: 0.148253\n",
      "Epoch [40/100], Step:  48000, d_loss: 0.411146, g_loss: 2.754634, real_scores: 0.865301, fake_scores: 0.111093\n",
      "Epoch [41/100], Step:  16000, d_loss: 0.432128, g_loss: 1.570537, real_scores: 0.898146, fake_scores: 0.204461\n",
      "Epoch [41/100], Step:  32000, d_loss: 0.367303, g_loss: 2.591732, real_scores: 0.908493, fake_scores: 0.145453\n",
      "Epoch [41/100], Step:  48000, d_loss: 0.274003, g_loss: 2.320132, real_scores: 0.897315, fake_scores: 0.088291\n",
      "Epoch [42/100], Step:  16000, d_loss: 0.550950, g_loss: 2.147652, real_scores: 0.812067, fake_scores: 0.153832\n",
      "Epoch [42/100], Step:  32000, d_loss: 0.455841, g_loss: 3.000795, real_scores: 0.883395, fake_scores: 0.151175\n",
      "Epoch [42/100], Step:  48000, d_loss: 0.537638, g_loss: 2.556440, real_scores: 0.819850, fake_scores: 0.111057\n",
      "Epoch [43/100], Step:  16000, d_loss: 0.475588, g_loss: 2.258148, real_scores: 0.850828, fake_scores: 0.138433\n",
      "Epoch [43/100], Step:  32000, d_loss: 0.513694, g_loss: 2.969048, real_scores: 0.803362, fake_scores: 0.081035\n",
      "Epoch [43/100], Step:  48000, d_loss: 0.332339, g_loss: 1.918180, real_scores: 0.950334, fake_scores: 0.222104\n",
      "Epoch [44/100], Step:  16000, d_loss: 0.409149, g_loss: 2.320560, real_scores: 0.902809, fake_scores: 0.179994\n",
      "Epoch [44/100], Step:  32000, d_loss: 0.388098, g_loss: 2.183789, real_scores: 0.947699, fake_scores: 0.238046\n",
      "Epoch [44/100], Step:  48000, d_loss: 0.298553, g_loss: 2.973958, real_scores: 0.888437, fake_scores: 0.043269\n",
      "Epoch [45/100], Step:  16000, d_loss: 0.409715, g_loss: 3.591974, real_scores: 0.830818, fake_scores: 0.061220\n",
      "Epoch [45/100], Step:  32000, d_loss: 0.396781, g_loss: 1.801934, real_scores: 0.886561, fake_scores: 0.137837\n",
      "Epoch [45/100], Step:  48000, d_loss: 0.453344, g_loss: 2.904859, real_scores: 0.855561, fake_scores: 0.062345\n",
      "Epoch [46/100], Step:  16000, d_loss: 0.734804, g_loss: 2.510547, real_scores: 0.732661, fake_scores: 0.132827\n",
      "Epoch [46/100], Step:  32000, d_loss: 0.321525, g_loss: 2.112477, real_scores: 0.981555, fake_scores: 0.222904\n",
      "Epoch [46/100], Step:  48000, d_loss: 0.683985, g_loss: 1.946740, real_scores: 0.800553, fake_scores: 0.160155\n",
      "Epoch [47/100], Step:  16000, d_loss: 0.400370, g_loss: 2.152420, real_scores: 0.926119, fake_scores: 0.199638\n",
      "Epoch [47/100], Step:  32000, d_loss: 0.476261, g_loss: 2.595913, real_scores: 0.827094, fake_scores: 0.079220\n",
      "Epoch [47/100], Step:  48000, d_loss: 0.529521, g_loss: 2.236260, real_scores: 0.859370, fake_scores: 0.193827\n",
      "Epoch [48/100], Step:  16000, d_loss: 0.687194, g_loss: 3.594475, real_scores: 0.830122, fake_scores: 0.069101\n",
      "Epoch [48/100], Step:  32000, d_loss: 0.402893, g_loss: 2.181088, real_scores: 0.869208, fake_scores: 0.129643\n",
      "Epoch [48/100], Step:  48000, d_loss: 0.370712, g_loss: 2.578054, real_scores: 0.881988, fake_scores: 0.083465\n",
      "Epoch [49/100], Step:  16000, d_loss: 0.333818, g_loss: 2.398752, real_scores: 0.874337, fake_scores: 0.082920\n",
      "Epoch [49/100], Step:  32000, d_loss: 0.582367, g_loss: 1.737701, real_scores: 0.805580, fake_scores: 0.163811\n",
      "Epoch [49/100], Step:  48000, d_loss: 0.425395, g_loss: 2.095813, real_scores: 0.852850, fake_scores: 0.148632\n",
      "Epoch [50/100], Step:  16000, d_loss: 0.366066, g_loss: 2.868936, real_scores: 0.875554, fake_scores: 0.119461\n",
      "Epoch [50/100], Step:  32000, d_loss: 0.484120, g_loss: 3.000418, real_scores: 0.861663, fake_scores: 0.123680\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/100], Step:  48000, d_loss: 0.489291, g_loss: 2.436440, real_scores: 0.869971, fake_scores: 0.097555\n",
      "Epoch [51/100], Step:  16000, d_loss: 0.413962, g_loss: 2.270707, real_scores: 0.884833, fake_scores: 0.183947\n",
      "Epoch [51/100], Step:  32000, d_loss: 0.462508, g_loss: 3.038633, real_scores: 0.839895, fake_scores: 0.067576\n",
      "Epoch [51/100], Step:  48000, d_loss: 0.397672, g_loss: 2.399278, real_scores: 0.891989, fake_scores: 0.130665\n",
      "Epoch [52/100], Step:  16000, d_loss: 0.572343, g_loss: 1.601494, real_scores: 0.834877, fake_scores: 0.229213\n",
      "Epoch [52/100], Step:  32000, d_loss: 0.316383, g_loss: 2.299988, real_scores: 0.899689, fake_scores: 0.124535\n",
      "Epoch [52/100], Step:  48000, d_loss: 0.319533, g_loss: 2.242620, real_scores: 0.888876, fake_scores: 0.124586\n",
      "Epoch [53/100], Step:  16000, d_loss: 0.549893, g_loss: 2.609358, real_scores: 0.809731, fake_scores: 0.087106\n",
      "Epoch [53/100], Step:  32000, d_loss: 0.387688, g_loss: 3.365582, real_scores: 0.877945, fake_scores: 0.064743\n",
      "Epoch [53/100], Step:  48000, d_loss: 0.352410, g_loss: 2.832886, real_scores: 0.903933, fake_scores: 0.124986\n",
      "Epoch [54/100], Step:  16000, d_loss: 0.584494, g_loss: 2.206324, real_scores: 0.903695, fake_scores: 0.260212\n",
      "Epoch [54/100], Step:  32000, d_loss: 0.491656, g_loss: 1.976859, real_scores: 0.885416, fake_scores: 0.214523\n",
      "Epoch [54/100], Step:  48000, d_loss: 0.431120, g_loss: 2.444145, real_scores: 0.834786, fake_scores: 0.082427\n",
      "Epoch [55/100], Step:  16000, d_loss: 0.435377, g_loss: 2.171395, real_scores: 0.915290, fake_scores: 0.206218\n",
      "Epoch [55/100], Step:  32000, d_loss: 0.321536, g_loss: 2.100529, real_scores: 0.925942, fake_scores: 0.154957\n",
      "Epoch [55/100], Step:  48000, d_loss: 0.521022, g_loss: 2.385704, real_scores: 0.827744, fake_scores: 0.093139\n",
      "Epoch [56/100], Step:  16000, d_loss: 0.350499, g_loss: 3.135274, real_scores: 0.904683, fake_scores: 0.085913\n",
      "Epoch [56/100], Step:  32000, d_loss: 0.340817, g_loss: 2.072540, real_scores: 0.933518, fake_scores: 0.199299\n",
      "Epoch [56/100], Step:  48000, d_loss: 0.467691, g_loss: 2.611856, real_scores: 0.792441, fake_scores: 0.068273\n",
      "Epoch [57/100], Step:  16000, d_loss: 0.680611, g_loss: 2.576821, real_scores: 0.768867, fake_scores: 0.079913\n",
      "Epoch [57/100], Step:  32000, d_loss: 0.589008, g_loss: 2.773934, real_scores: 0.794817, fake_scores: 0.103465\n",
      "Epoch [57/100], Step:  48000, d_loss: 0.437833, g_loss: 2.327176, real_scores: 0.862430, fake_scores: 0.099117\n",
      "Epoch [58/100], Step:  16000, d_loss: 0.459702, g_loss: 2.967789, real_scores: 0.831531, fake_scores: 0.083838\n",
      "Epoch [58/100], Step:  32000, d_loss: 0.455616, g_loss: 2.263029, real_scores: 0.855586, fake_scores: 0.138203\n",
      "Epoch [58/100], Step:  48000, d_loss: 0.592199, g_loss: 2.812595, real_scores: 0.809657, fake_scores: 0.069528\n",
      "Epoch [59/100], Step:  16000, d_loss: 0.475325, g_loss: 2.454777, real_scores: 0.853192, fake_scores: 0.113707\n",
      "Epoch [59/100], Step:  32000, d_loss: 0.193915, g_loss: 2.403867, real_scores: 0.949324, fake_scores: 0.101116\n",
      "Epoch [59/100], Step:  48000, d_loss: 0.670087, g_loss: 2.032676, real_scores: 0.764461, fake_scores: 0.139895\n",
      "Epoch [60/100], Step:  16000, d_loss: 0.652098, g_loss: 2.050261, real_scores: 0.777206, fake_scores: 0.164942\n",
      "Epoch [60/100], Step:  32000, d_loss: 0.214577, g_loss: 2.308037, real_scores: 0.956140, fake_scores: 0.119909\n",
      "Epoch [60/100], Step:  48000, d_loss: 0.450617, g_loss: 2.289317, real_scores: 0.878422, fake_scores: 0.159678\n",
      "Epoch [61/100], Step:  16000, d_loss: 0.290904, g_loss: 3.200559, real_scores: 0.916062, fake_scores: 0.079256\n",
      "Epoch [61/100], Step:  32000, d_loss: 0.421406, g_loss: 2.383286, real_scores: 0.870099, fake_scores: 0.128786\n",
      "Epoch [61/100], Step:  48000, d_loss: 0.610988, g_loss: 2.482276, real_scores: 0.806133, fake_scores: 0.053488\n",
      "Epoch [62/100], Step:  16000, d_loss: 0.414515, g_loss: 2.041143, real_scores: 0.852962, fake_scores: 0.094214\n",
      "Epoch [62/100], Step:  32000, d_loss: 0.205974, g_loss: 2.930721, real_scores: 0.956331, fake_scores: 0.115553\n",
      "Epoch [62/100], Step:  48000, d_loss: 0.514233, g_loss: 2.364535, real_scores: 0.806634, fake_scores: 0.109985\n",
      "Epoch [63/100], Step:  16000, d_loss: 0.355799, g_loss: 2.439930, real_scores: 0.906949, fake_scores: 0.157759\n",
      "Epoch [63/100], Step:  32000, d_loss: 0.402323, g_loss: 2.596955, real_scores: 0.869993, fake_scores: 0.099278\n",
      "Epoch [63/100], Step:  48000, d_loss: 0.427926, g_loss: 2.628307, real_scores: 0.862702, fake_scores: 0.091727\n",
      "Epoch [64/100], Step:  16000, d_loss: 0.469129, g_loss: 2.374516, real_scores: 0.872779, fake_scores: 0.168685\n",
      "Epoch [64/100], Step:  32000, d_loss: 0.950165, g_loss: 2.508155, real_scores: 0.678041, fake_scores: 0.069180\n",
      "Epoch [64/100], Step:  48000, d_loss: 0.360999, g_loss: 2.800866, real_scores: 0.859564, fake_scores: 0.058621\n",
      "Epoch [65/100], Step:  16000, d_loss: 0.436568, g_loss: 1.900858, real_scores: 0.901627, fake_scores: 0.201873\n",
      "Epoch [65/100], Step:  32000, d_loss: 0.276822, g_loss: 2.482031, real_scores: 0.943598, fake_scores: 0.121313\n",
      "Epoch [65/100], Step:  48000, d_loss: 0.484723, g_loss: 1.988198, real_scores: 0.884170, fake_scores: 0.223932\n",
      "Epoch [66/100], Step:  16000, d_loss: 0.267076, g_loss: 1.871870, real_scores: 0.963083, fake_scores: 0.161668\n",
      "Epoch [66/100], Step:  32000, d_loss: 0.282253, g_loss: 2.229434, real_scores: 0.914993, fake_scores: 0.109058\n",
      "Epoch [66/100], Step:  48000, d_loss: 0.542378, g_loss: 1.937191, real_scores: 0.844082, fake_scores: 0.158300\n",
      "Epoch [67/100], Step:  16000, d_loss: 0.509053, g_loss: 3.181298, real_scores: 0.828448, fake_scores: 0.101850\n",
      "Epoch [67/100], Step:  32000, d_loss: 0.427725, g_loss: 2.542741, real_scores: 0.856425, fake_scores: 0.115325\n",
      "Epoch [67/100], Step:  48000, d_loss: 0.263924, g_loss: 2.516342, real_scores: 0.925355, fake_scores: 0.134353\n",
      "Epoch [68/100], Step:  16000, d_loss: 0.494249, g_loss: 1.888237, real_scores: 0.837053, fake_scores: 0.114922\n",
      "Epoch [68/100], Step:  32000, d_loss: 0.364461, g_loss: 2.575735, real_scores: 0.872755, fake_scores: 0.108957\n",
      "Epoch [68/100], Step:  48000, d_loss: 0.287554, g_loss: 2.726934, real_scores: 0.930999, fake_scores: 0.131992\n",
      "Epoch [69/100], Step:  16000, d_loss: 0.422263, g_loss: 2.500755, real_scores: 0.867598, fake_scores: 0.155146\n",
      "Epoch [69/100], Step:  32000, d_loss: 0.497752, g_loss: 2.654305, real_scores: 0.830671, fake_scores: 0.068676\n",
      "Epoch [69/100], Step:  48000, d_loss: 0.311254, g_loss: 2.388884, real_scores: 0.922086, fake_scores: 0.157631\n",
      "Epoch [70/100], Step:  16000, d_loss: 0.367210, g_loss: 2.919056, real_scores: 0.883413, fake_scores: 0.065429\n",
      "Epoch [70/100], Step:  32000, d_loss: 0.706974, g_loss: 2.921131, real_scores: 0.771080, fake_scores: 0.086359\n",
      "Epoch [70/100], Step:  48000, d_loss: 0.426218, g_loss: 3.065935, real_scores: 0.854304, fake_scores: 0.058432\n",
      "Epoch [71/100], Step:  16000, d_loss: 0.435086, g_loss: 2.554835, real_scores: 0.844967, fake_scores: 0.065684\n",
      "Epoch [71/100], Step:  32000, d_loss: 0.395751, g_loss: 2.385932, real_scores: 0.884517, fake_scores: 0.144989\n",
      "Epoch [71/100], Step:  48000, d_loss: 0.435479, g_loss: 2.132288, real_scores: 0.877978, fake_scores: 0.147967\n",
      "Epoch [72/100], Step:  16000, d_loss: 0.440781, g_loss: 2.110673, real_scores: 0.903018, fake_scores: 0.212525\n",
      "Epoch [72/100], Step:  32000, d_loss: 0.231990, g_loss: 2.706658, real_scores: 0.944960, fake_scores: 0.103655\n",
      "Epoch [72/100], Step:  48000, d_loss: 0.614137, g_loss: 2.232864, real_scores: 0.795900, fake_scores: 0.146568\n",
      "Epoch [73/100], Step:  16000, d_loss: 0.406042, g_loss: 2.089859, real_scores: 0.857539, fake_scores: 0.117063\n",
      "Epoch [73/100], Step:  32000, d_loss: 0.268085, g_loss: 2.488934, real_scores: 0.915157, fake_scores: 0.101460\n",
      "Epoch [73/100], Step:  48000, d_loss: 0.374195, g_loss: 2.283197, real_scores: 0.890987, fake_scores: 0.139160\n",
      "Epoch [74/100], Step:  16000, d_loss: 0.472477, g_loss: 3.326556, real_scores: 0.872055, fake_scores: 0.091136\n",
      "Epoch [74/100], Step:  32000, d_loss: 0.456812, g_loss: 2.897184, real_scores: 0.838543, fake_scores: 0.076893\n",
      "Epoch [74/100], Step:  48000, d_loss: 0.321124, g_loss: 2.210788, real_scores: 0.910556, fake_scores: 0.137011\n",
      "Epoch [75/100], Step:  16000, d_loss: 0.256207, g_loss: 2.375612, real_scores: 0.929309, fake_scores: 0.114838\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [75/100], Step:  32000, d_loss: 0.414587, g_loss: 2.389448, real_scores: 0.866606, fake_scores: 0.124409\n",
      "Epoch [75/100], Step:  48000, d_loss: 0.494127, g_loss: 2.121598, real_scores: 0.836678, fake_scores: 0.137126\n",
      "Epoch [76/100], Step:  16000, d_loss: 0.545136, g_loss: 2.652172, real_scores: 0.816078, fake_scores: 0.113644\n",
      "Epoch [76/100], Step:  32000, d_loss: 0.349267, g_loss: 1.937458, real_scores: 0.891234, fake_scores: 0.118762\n",
      "Epoch [76/100], Step:  48000, d_loss: 0.446829, g_loss: 3.377032, real_scores: 0.865556, fake_scores: 0.073076\n",
      "Epoch [77/100], Step:  16000, d_loss: 0.547785, g_loss: 2.310656, real_scores: 0.826981, fake_scores: 0.127553\n",
      "Epoch [77/100], Step:  32000, d_loss: 0.508228, g_loss: 3.385291, real_scores: 0.857941, fake_scores: 0.089290\n",
      "Epoch [77/100], Step:  48000, d_loss: 0.452473, g_loss: 2.667303, real_scores: 0.858265, fake_scores: 0.074712\n",
      "Epoch [78/100], Step:  16000, d_loss: 0.375978, g_loss: 2.523567, real_scores: 0.857072, fake_scores: 0.090194\n",
      "Epoch [78/100], Step:  32000, d_loss: 0.233318, g_loss: 2.790053, real_scores: 0.947257, fake_scores: 0.088295\n",
      "Epoch [78/100], Step:  48000, d_loss: 0.367731, g_loss: 2.488589, real_scores: 0.874170, fake_scores: 0.097351\n",
      "Epoch [79/100], Step:  16000, d_loss: 0.571061, g_loss: 2.087877, real_scores: 0.752663, fake_scores: 0.099966\n",
      "Epoch [79/100], Step:  32000, d_loss: 0.335150, g_loss: 1.927029, real_scores: 0.932197, fake_scores: 0.190615\n",
      "Epoch [79/100], Step:  48000, d_loss: 0.450799, g_loss: 2.723922, real_scores: 0.882417, fake_scores: 0.104460\n",
      "Epoch [80/100], Step:  16000, d_loss: 0.466932, g_loss: 2.298876, real_scores: 0.868018, fake_scores: 0.146835\n",
      "Epoch [80/100], Step:  32000, d_loss: 0.321029, g_loss: 2.424397, real_scores: 0.892612, fake_scores: 0.077900\n",
      "Epoch [80/100], Step:  48000, d_loss: 0.671331, g_loss: 2.726490, real_scores: 0.784593, fake_scores: 0.095792\n",
      "Epoch [81/100], Step:  16000, d_loss: 0.471000, g_loss: 2.504574, real_scores: 0.861477, fake_scores: 0.150492\n",
      "Epoch [81/100], Step:  32000, d_loss: 0.411491, g_loss: 2.636989, real_scores: 0.839833, fake_scores: 0.078653\n",
      "Epoch [81/100], Step:  48000, d_loss: 0.292563, g_loss: 2.653777, real_scores: 0.913285, fake_scores: 0.106604\n",
      "Epoch [82/100], Step:  16000, d_loss: 0.346215, g_loss: 1.922943, real_scores: 0.912501, fake_scores: 0.159901\n",
      "Epoch [82/100], Step:  32000, d_loss: 0.271334, g_loss: 2.618178, real_scores: 0.918190, fake_scores: 0.117332\n",
      "Epoch [82/100], Step:  48000, d_loss: 0.335050, g_loss: 2.353464, real_scores: 0.887235, fake_scores: 0.113961\n",
      "Epoch [83/100], Step:  16000, d_loss: 0.128530, g_loss: 3.140282, real_scores: 0.967740, fake_scores: 0.063503\n",
      "Epoch [83/100], Step:  32000, d_loss: 0.412776, g_loss: 2.676519, real_scores: 0.856758, fake_scores: 0.087790\n",
      "Epoch [83/100], Step:  48000, d_loss: 0.235726, g_loss: 2.595332, real_scores: 0.939461, fake_scores: 0.100423\n",
      "Epoch [84/100], Step:  16000, d_loss: 0.520319, g_loss: 2.394263, real_scores: 0.825861, fake_scores: 0.127018\n",
      "Epoch [84/100], Step:  32000, d_loss: 0.395206, g_loss: 2.867124, real_scores: 0.882075, fake_scores: 0.104979\n",
      "Epoch [84/100], Step:  48000, d_loss: 0.506567, g_loss: 2.451962, real_scores: 0.816210, fake_scores: 0.119194\n",
      "Epoch [85/100], Step:  16000, d_loss: 0.445597, g_loss: 2.816052, real_scores: 0.841296, fake_scores: 0.084792\n",
      "Epoch [85/100], Step:  32000, d_loss: 0.374820, g_loss: 2.216301, real_scores: 0.877517, fake_scores: 0.104561\n",
      "Epoch [85/100], Step:  48000, d_loss: 0.759140, g_loss: 2.571193, real_scores: 0.712387, fake_scores: 0.062717\n",
      "Epoch [86/100], Step:  16000, d_loss: 0.434648, g_loss: 1.932118, real_scores: 0.875074, fake_scores: 0.195102\n",
      "Epoch [86/100], Step:  32000, d_loss: 0.299062, g_loss: 2.756009, real_scores: 0.907275, fake_scores: 0.103853\n",
      "Epoch [86/100], Step:  48000, d_loss: 0.369835, g_loss: 2.315982, real_scores: 0.869527, fake_scores: 0.091201\n",
      "Epoch [87/100], Step:  16000, d_loss: 0.679906, g_loss: 3.300350, real_scores: 0.749570, fake_scores: 0.046160\n",
      "Epoch [87/100], Step:  32000, d_loss: 0.328101, g_loss: 2.750749, real_scores: 0.907456, fake_scores: 0.061827\n",
      "Epoch [87/100], Step:  48000, d_loss: 0.431920, g_loss: 2.555843, real_scores: 0.848315, fake_scores: 0.096998\n",
      "Epoch [88/100], Step:  16000, d_loss: 0.422268, g_loss: 2.088527, real_scores: 0.857358, fake_scores: 0.163102\n",
      "Epoch [88/100], Step:  32000, d_loss: 0.256505, g_loss: 2.521683, real_scores: 0.913677, fake_scores: 0.080608\n",
      "Epoch [88/100], Step:  48000, d_loss: 0.692919, g_loss: 2.837319, real_scores: 0.782060, fake_scores: 0.083731\n",
      "Epoch [89/100], Step:  16000, d_loss: 0.543025, g_loss: 2.392454, real_scores: 0.831167, fake_scores: 0.132318\n",
      "Epoch [89/100], Step:  32000, d_loss: 0.419396, g_loss: 2.554526, real_scores: 0.864761, fake_scores: 0.118917\n",
      "Epoch [89/100], Step:  48000, d_loss: 0.318285, g_loss: 2.612158, real_scores: 0.887253, fake_scores: 0.111052\n",
      "Epoch [90/100], Step:  16000, d_loss: 0.348310, g_loss: 2.278647, real_scores: 0.900099, fake_scores: 0.138948\n",
      "Epoch [90/100], Step:  32000, d_loss: 0.361341, g_loss: 3.099122, real_scores: 0.863555, fake_scores: 0.067880\n",
      "Epoch [90/100], Step:  48000, d_loss: 0.375880, g_loss: 2.852077, real_scores: 0.862890, fake_scores: 0.088121\n",
      "Epoch [91/100], Step:  16000, d_loss: 0.478002, g_loss: 2.878286, real_scores: 0.823286, fake_scores: 0.099923\n",
      "Epoch [91/100], Step:  32000, d_loss: 0.354322, g_loss: 2.282557, real_scores: 0.909306, fake_scores: 0.150087\n",
      "Epoch [91/100], Step:  48000, d_loss: 0.397444, g_loss: 2.233231, real_scores: 0.875809, fake_scores: 0.133296\n",
      "Epoch [92/100], Step:  16000, d_loss: 0.679874, g_loss: 2.976840, real_scores: 0.790141, fake_scores: 0.092474\n",
      "Epoch [92/100], Step:  32000, d_loss: 0.378491, g_loss: 3.056441, real_scores: 0.868906, fake_scores: 0.079399\n",
      "Epoch [92/100], Step:  48000, d_loss: 0.459033, g_loss: 2.448428, real_scores: 0.815951, fake_scores: 0.087171\n",
      "Epoch [93/100], Step:  16000, d_loss: 0.343023, g_loss: 2.241970, real_scores: 0.888585, fake_scores: 0.129082\n",
      "Epoch [93/100], Step:  32000, d_loss: 0.341622, g_loss: 3.000316, real_scores: 0.871387, fake_scores: 0.070917\n",
      "Epoch [93/100], Step:  48000, d_loss: 0.406312, g_loss: 2.912730, real_scores: 0.852944, fake_scores: 0.070231\n",
      "Epoch [94/100], Step:  16000, d_loss: 0.445273, g_loss: 2.821165, real_scores: 0.856613, fake_scores: 0.120436\n",
      "Epoch [94/100], Step:  32000, d_loss: 0.351194, g_loss: 2.385479, real_scores: 0.888734, fake_scores: 0.131592\n",
      "Epoch [94/100], Step:  48000, d_loss: 0.344534, g_loss: 2.162109, real_scores: 0.884672, fake_scores: 0.153084\n",
      "Epoch [95/100], Step:  16000, d_loss: 0.222769, g_loss: 3.088347, real_scores: 0.949302, fake_scores: 0.097411\n",
      "Epoch [95/100], Step:  32000, d_loss: 0.310191, g_loss: 2.002326, real_scores: 0.944758, fake_scores: 0.164947\n",
      "Epoch [95/100], Step:  48000, d_loss: 0.260256, g_loss: 2.866710, real_scores: 0.931659, fake_scores: 0.104805\n",
      "Epoch [96/100], Step:  16000, d_loss: 0.403577, g_loss: 3.352888, real_scores: 0.883069, fake_scores: 0.107783\n",
      "Epoch [96/100], Step:  32000, d_loss: 0.263897, g_loss: 2.404224, real_scores: 0.917243, fake_scores: 0.103964\n",
      "Epoch [96/100], Step:  48000, d_loss: 0.418740, g_loss: 2.753648, real_scores: 0.855589, fake_scores: 0.087562\n",
      "Epoch [97/100], Step:  16000, d_loss: 0.310222, g_loss: 2.895534, real_scores: 0.879075, fake_scores: 0.070005\n",
      "Epoch [97/100], Step:  32000, d_loss: 0.355748, g_loss: 2.898151, real_scores: 0.897404, fake_scores: 0.119313\n",
      "Epoch [97/100], Step:  48000, d_loss: 0.291576, g_loss: 2.384948, real_scores: 0.905907, fake_scores: 0.114978\n",
      "Epoch [98/100], Step:  16000, d_loss: 0.376882, g_loss: 2.684642, real_scores: 0.874243, fake_scores: 0.124939\n",
      "Epoch [98/100], Step:  32000, d_loss: 0.330896, g_loss: 2.538151, real_scores: 0.881042, fake_scores: 0.086913\n",
      "Epoch [98/100], Step:  48000, d_loss: 0.181838, g_loss: 2.555941, real_scores: 0.953712, fake_scores: 0.096391\n",
      "Epoch [99/100], Step:  16000, d_loss: 0.421683, g_loss: 2.170981, real_scores: 0.858551, fake_scores: 0.107960\n",
      "Epoch [99/100], Step:  32000, d_loss: 0.374053, g_loss: 2.623942, real_scores: 0.872549, fake_scores: 0.133625\n",
      "Epoch [99/100], Step:  48000, d_loss: 0.297012, g_loss: 2.613091, real_scores: 0.908776, fake_scores: 0.128456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/100], Step:  16000, d_loss: 0.166139, g_loss: 2.225789, real_scores: 0.979476, fake_scores: 0.123475\n",
      "Epoch [100/100], Step:  32000, d_loss: 0.387946, g_loss: 2.721331, real_scores: 0.860175, fake_scores: 0.074702\n",
      "Epoch [100/100], Step:  48000, d_loss: 0.254766, g_loss: 2.280879, real_scores: 0.926064, fake_scores: 0.111063\n"
     ]
    }
   ],
   "source": [
    "total_count = len(dataloader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (img, _) in enumerate(dataloader):\n",
    "        \n",
    "        real_img = img.cuda()\n",
    "        real_labels = torch.ones(img.size(0), 1).cuda()\n",
    "        fake_labels = torch.zeros(img.size(0), 1).cuda()\n",
    "        \n",
    "        real_out = d(real_img)\n",
    "        d_loss_real = criterion(real_out, real_labels)\n",
    "        real_scores = real_out\n",
    "        \n",
    "        z = torch.randn(img.size(0), z_dimension).cuda()\n",
    "        fake_img = g(z)\n",
    "        fake_out = d(fake_img)\n",
    "        d_loss_fake = criterion(fake_out, fake_labels)\n",
    "        fake_scores = fake_out\n",
    "        \n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "        d_optimezer.zero_grad()\n",
    "        d_loss.backward()\n",
    "        d_optimezer.step()\n",
    "        \n",
    "        z = torch.randn(img.size(0), z_dimension).cuda()\n",
    "        fake_img = g(z)\n",
    "        fake_out = d(fake_img)\n",
    "        g_loss = criterion(fake_out, real_labels)\n",
    "        \n",
    "        g_optimezer.zero_grad()\n",
    "        g_loss.backward()\n",
    "        g_optimezer.step()\n",
    "        \n",
    "        step = epoch * total_count + i + 1\n",
    "        \n",
    "        writer.add_scalar('Discriminator Real Loss', d_loss_real.item(), step)\n",
    "        writer.add_scalar('Discriminator Fake Loss', d_loss_fake.item(), step)\n",
    "        writer.add_scalar('Discriminator Loss', d_loss.item(), step)\n",
    "        writer.add_scalar('Generator Loss', g_loss.item(), step)\n",
    "        \n",
    "        \n",
    "        if (i + 1) % 500 == 0:\n",
    "            print 'Epoch [{}/{}], Step: {:6d}, d_loss: {:.6f}, g_loss: {:.6f}, real_scores: {:.6f}' \\\n",
    "', fake_scores: {:.6f}'.format(epoch+1, num_epochs, (i+1) * batch_size, d_loss, g_loss, real_scores.mean(), fake_scores.mean())\n",
    "    \n",
    "    if epoch == 0:\n",
    "        real_images = real_img.view(-1, 1, 28, 28).cpu().data\n",
    "        save_image(real_images, './cnn_gan_img/real_images.png')\n",
    "\n",
    "    fake_images = fake_img.view(-1, 1, 28, 28).cpu().data\n",
    "    save_image(fake_images, './cnn_gan_img/fake_images-{}.png'.format(epoch+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(d.state_dict(), './ser/discriminator.pkl')\n",
    "torch.save(g.state_dict(), './ser/generator.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAB8CAYAAABnjns5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f922ecb4a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "z = torch.randn(4, z_dimension).to(device)\n",
    "images = g(z)\n",
    "# save_image(images, 'xx.png')\n",
    "plt.imshow(Image.fromarray(make_grid(images).mul(255).clamp(0, 255).byte().permute(1, 2, 0).cpu().numpy()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
